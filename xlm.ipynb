{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09f58533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ce3233e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"import torch\\ntorch.manual_seed(0)\\ntorch.use_deterministic_algorithms(False)\\n\\nimport random\\nrandom.seed(0)\\n\\nimport numpy as np\\nnp.random.seed(0)\";\n",
       "                var nbb_formatted_code = \"import torch\\n\\ntorch.manual_seed(0)\\ntorch.use_deterministic_algorithms(False)\\n\\nimport random\\n\\nrandom.seed(0)\\n\\nimport numpy as np\\n\\nnp.random.seed(0)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)\n",
    "torch.use_deterministic_algorithms(False)\n",
    "\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6eb0c6c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"if torch.cuda.is_available():\\n    dev = \\\"cuda:1\\\"\\nelse:\\n    dev = \\\"cpu\\\"\\nDEVICE = torch.device(dev)\";\n",
       "                var nbb_formatted_code = \"if torch.cuda.is_available():\\n    dev = \\\"cuda:1\\\"\\nelse:\\n    dev = \\\"cpu\\\"\\nDEVICE = torch.device(dev)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:1\"\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "DEVICE = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "579ddd2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"import torch\\nfrom transformers import XLMTokenizer, XLMWithLMHeadModel\\n\\n# tokenizer = XLMTokenizer.from_pretrained(\\\"xlm-mlm-100-1280\\\")\\n# model = XLMWithLMHeadModel.from_pretrained(\\\"xlm-mlm-100-1280\\\")\";\n",
       "                var nbb_formatted_code = \"import torch\\nfrom transformers import XLMTokenizer, XLMWithLMHeadModel\\n\\n# tokenizer = XLMTokenizer.from_pretrained(\\\"xlm-mlm-100-1280\\\")\\n# model = XLMWithLMHeadModel.from_pretrained(\\\"xlm-mlm-100-1280\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import XLMTokenizer, XLMWithLMHeadModel\n",
    "\n",
    "# tokenizer = XLMTokenizer.from_pretrained(\"xlm-mlm-100-1280\")\n",
    "# model = XLMWithLMHeadModel.from_pretrained(\"xlm-mlm-100-1280\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cb99364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"from transformers import AutoTokenizer, AutoModelForSequenceClassification\\n\\n# model = AutoModelForSequenceClassification.from_pretrained(\\\"xlm-mlm-100-1280\\\", num_labels=1)\";\n",
       "                var nbb_formatted_code = \"from transformers import AutoTokenizer, AutoModelForSequenceClassification\\n\\n# model = AutoModelForSequenceClassification.from_pretrained(\\\"xlm-mlm-100-1280\\\", num_labels=1)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"xlm-mlm-100-1280\", num_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fa29423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"import torch.nn as nn\\n\\n\\nclass MultiLingualModel(nn.Module):\\n    def __init__(self, model_name):\\n        super().__init__()\\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\\n        self.model = AutoModelForSequenceClassification.from_pretrained(\\n            model_name, num_labels=1\\n        ).to(DEVICE)\\n\\n    def forward(self, sentences):\\n        encoded_input = self.tokenizer(\\n            sentences, padding=True, truncation=True, return_tensors=\\\"pt\\\"\\n        ).to(DEVICE)\\n        out = self.model(encoded_input[\\\"input_ids\\\"])\\n        return out\";\n",
       "                var nbb_formatted_code = \"import torch.nn as nn\\n\\n\\nclass MultiLingualModel(nn.Module):\\n    def __init__(self, model_name):\\n        super().__init__()\\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\\n        self.model = AutoModelForSequenceClassification.from_pretrained(\\n            model_name, num_labels=1\\n        ).to(DEVICE)\\n\\n    def forward(self, sentences):\\n        encoded_input = self.tokenizer(\\n            sentences, padding=True, truncation=True, return_tensors=\\\"pt\\\"\\n        ).to(DEVICE)\\n        out = self.model(encoded_input[\\\"input_ids\\\"])\\n        return out\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MultiLingualModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name, num_labels=1\n",
    "        ).to(DEVICE)\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        encoded_input = self.tokenizer(\n",
    "            sentences, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "        ).to(DEVICE)\n",
    "        out = self.model(encoded_input[\"input_ids\"])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b8d18480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 93;\n",
       "                var nbb_unformatted_code = \"model = MultiLingualModel(\\\"cardiffnlp/twitter-xlm-roberta-base\\\")\";\n",
       "                var nbb_formatted_code = \"model = MultiLingualModel(\\\"cardiffnlp/twitter-xlm-roberta-base\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = MultiLingualModel(\"cardiffnlp/twitter-xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ef5aaa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"out = model([\\\"Wikipedia was used to\\\", \\\"This is great\\\"])\";\n",
       "                var nbb_formatted_code = \"out = model([\\\"Wikipedia was used to\\\", \\\"This is great\\\"])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out = model([\"Wikipedia was used to\", \"This is great\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cddfce6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"del out\";\n",
       "                var nbb_formatted_code = \"del out\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "del out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "371a16b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"with torch.cuda.device(\\\"cuda:1\\\"):\\n    torch.cuda.empty_cache()\";\n",
       "                var nbb_formatted_code = \"with torch.cuda.device(\\\"cuda:1\\\"):\\n    torch.cuda.empty_cache()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.cuda.device(\"cuda:1\"):\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a8979c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"import pandas as pd\\n\\ndata = pd.read_csv(\\\"data/train.csv\\\")\";\n",
       "                var nbb_formatted_code = \"import pandas as pd\\n\\ndata = pd.read_csv(\\\"data/train.csv\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ddb457c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"def get_data_loader(data, batch_size=16):\\n    shuffled_data = data.sample(frac=1, random_state=0).reset_index(drop=True)\\n    start = 0\\n    end = start + batch_size\\n    data_len = len(shuffled_data)\\n    while start < data_len:\\n        sub_data = shuffled_data[start:end]\\n        start += batch_size\\n        end = min(start + batch_size, data_len)\\n        yield sub_data[\\\"text\\\"].tolist(), torch.tensor(sub_data[\\\"label\\\"].tolist())\";\n",
       "                var nbb_formatted_code = \"def get_data_loader(data, batch_size=16):\\n    shuffled_data = data.sample(frac=1, random_state=0).reset_index(drop=True)\\n    start = 0\\n    end = start + batch_size\\n    data_len = len(shuffled_data)\\n    while start < data_len:\\n        sub_data = shuffled_data[start:end]\\n        start += batch_size\\n        end = min(start + batch_size, data_len)\\n        yield sub_data[\\\"text\\\"].tolist(), torch.tensor(sub_data[\\\"label\\\"].tolist())\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_data_loader(data, batch_size=16):\n",
    "    shuffled_data = data.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "    start = 0\n",
    "    end = start + batch_size\n",
    "    data_len = len(shuffled_data)\n",
    "    while start < data_len:\n",
    "        sub_data = shuffled_data[start:end]\n",
    "        start += batch_size\n",
    "        end = min(start + batch_size, data_len)\n",
    "        yield sub_data[\"text\"].tolist(), torch.tensor(sub_data[\"label\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3aa6f2c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"from sklearn.model_selection import train_test_split\\n\\ntrain_data, valid_data = train_test_split(\\n    data, test_size=0.2, shuffle=True, random_state=0\\n)\";\n",
       "                var nbb_formatted_code = \"from sklearn.model_selection import train_test_split\\n\\ntrain_data, valid_data = train_test_split(\\n    data, test_size=0.2, shuffle=True, random_state=0\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, valid_data = train_test_split(\n",
    "    data, test_size=0.2, shuffle=True, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "856ec845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 105;\n",
       "                var nbb_unformatted_code = \"from tqdm import tqdm\\n\\n\\ndef train_or_valid(model_args, curr_epoch, model, is_train=True):\\n    \\\"\\\"\\\"\\n    This fn. is used to train or validate the model\\n    params:\\n        model_args: a dict of model parameters\\n        curr_epoch: Current value of the epoch\\n        model: model to be trained\\n        is_train: can be True or False depending on whether to train or validate\\n\\n    returns:\\n        loss: sum of the loss across all tokens\\n\\n    \\\"\\\"\\\"\\n    loss_list = []\\n    y_pred_list = []\\n    y_list = []\\n    model_args[\\\"optimizer\\\"].zero_grad()\\n    if is_train:\\n        data_loader = get_data_loader(train_data)\\n        model.train()\\n    else:\\n        data_loader = get_data_loader(valid_data)\\n        model.eval()\\n\\n    with tqdm(data_loader, unit=\\\"batch\\\") as tepoch:\\n        tepoch.set_description(f\\\"Epoch {curr_epoch}\\\")\\n        for step, batch in enumerate(tepoch):\\n            X = batch[0]\\n            y = batch[1].float().to(DEVICE)\\n            y_pred = model(X)\\n            y_pred_list.extend(y_pred[\\\"logits\\\"].reshape(-1).tolist())\\n            y_list.extend(y.tolist())\\n            loss = model_args[\\\"criterion\\\"](y_pred[\\\"logits\\\"].reshape(-1), y)\\n            loss_list.append(loss.item())\\n            if is_train:\\n                model_args[\\\"optimizer\\\"].zero_grad()\\n                loss.backward()\\n                nn.utils.clip_grad_norm_(model.parameters(), 2)\\n                model_args[\\\"optimizer\\\"].step()\\n            tepoch.set_postfix(loss=loss.item())\\n\\n    return sum(loss_list), y_pred_list, y_list\";\n",
       "                var nbb_formatted_code = \"from tqdm import tqdm\\n\\n\\ndef train_or_valid(model_args, curr_epoch, model, is_train=True):\\n    \\\"\\\"\\\"\\n    This fn. is used to train or validate the model\\n    params:\\n        model_args: a dict of model parameters\\n        curr_epoch: Current value of the epoch\\n        model: model to be trained\\n        is_train: can be True or False depending on whether to train or validate\\n\\n    returns:\\n        loss: sum of the loss across all tokens\\n\\n    \\\"\\\"\\\"\\n    loss_list = []\\n    y_pred_list = []\\n    y_list = []\\n    model_args[\\\"optimizer\\\"].zero_grad()\\n    if is_train:\\n        data_loader = get_data_loader(train_data)\\n        model.train()\\n    else:\\n        data_loader = get_data_loader(valid_data)\\n        model.eval()\\n\\n    with tqdm(data_loader, unit=\\\"batch\\\") as tepoch:\\n        tepoch.set_description(f\\\"Epoch {curr_epoch}\\\")\\n        for step, batch in enumerate(tepoch):\\n            X = batch[0]\\n            y = batch[1].float().to(DEVICE)\\n            y_pred = model(X)\\n            y_pred_list.extend(y_pred[\\\"logits\\\"].reshape(-1).tolist())\\n            y_list.extend(y.tolist())\\n            loss = model_args[\\\"criterion\\\"](y_pred[\\\"logits\\\"].reshape(-1), y)\\n            loss_list.append(loss.item())\\n            if is_train:\\n                model_args[\\\"optimizer\\\"].zero_grad()\\n                loss.backward()\\n                nn.utils.clip_grad_norm_(model.parameters(), 2)\\n                model_args[\\\"optimizer\\\"].step()\\n            tepoch.set_postfix(loss=loss.item())\\n\\n    return sum(loss_list), y_pred_list, y_list\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train_or_valid(model_args, curr_epoch, model, is_train=True):\n",
    "    \"\"\"\n",
    "    This fn. is used to train or validate the model\n",
    "    params:\n",
    "        model_args: a dict of model parameters\n",
    "        curr_epoch: Current value of the epoch\n",
    "        model: model to be trained\n",
    "        is_train: can be True or False depending on whether to train or validate\n",
    "\n",
    "    returns:\n",
    "        loss: sum of the loss across all tokens\n",
    "\n",
    "    \"\"\"\n",
    "    loss_list = []\n",
    "    y_pred_list = []\n",
    "    y_list = []\n",
    "    model_args[\"optimizer\"].zero_grad()\n",
    "    if is_train:\n",
    "        data_loader = get_data_loader(train_data)\n",
    "        model.train()\n",
    "    else:\n",
    "        data_loader = get_data_loader(valid_data)\n",
    "        model.eval()\n",
    "\n",
    "    with tqdm(data_loader, unit=\"batch\") as tepoch:\n",
    "        tepoch.set_description(f\"Epoch {curr_epoch}\")\n",
    "        for step, batch in enumerate(tepoch):\n",
    "            X = batch[0]\n",
    "            y = batch[1].float().to(DEVICE)\n",
    "            y_pred = model(X)\n",
    "            y_pred_list.extend(y_pred[\"logits\"].reshape(-1).tolist())\n",
    "            y_list.extend(y.tolist())\n",
    "            loss = model_args[\"criterion\"](y_pred[\"logits\"].reshape(-1), y)\n",
    "            loss_list.append(loss.item())\n",
    "            if is_train:\n",
    "                model_args[\"optimizer\"].zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), 2)\n",
    "                model_args[\"optimizer\"].step()\n",
    "            tepoch.set_postfix(loss=loss.item())\n",
    "\n",
    "    return sum(loss_list), y_pred_list, y_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "60dadd1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 110;\n",
       "                var nbb_unformatted_code = \"# Defining parameters for the model\\ndef get_model_args():\\n    # returns a dict - {param: value}\\n    return {\\n        \\\"batch_size\\\": 16,\\n        \\\"epoch\\\": 5,\\n        \\\"learning_rate\\\": 0.001,\\n    }\";\n",
       "                var nbb_formatted_code = \"# Defining parameters for the model\\ndef get_model_args():\\n    # returns a dict - {param: value}\\n    return {\\n        \\\"batch_size\\\": 16,\\n        \\\"epoch\\\": 5,\\n        \\\"learning_rate\\\": 0.001,\\n    }\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Defining parameters for the model\n",
    "def get_model_args():\n",
    "    # returns a dict - {param: value}\n",
    "    return {\n",
    "        \"batch_size\": 16,\n",
    "        \"epoch\": 5,\n",
    "        \"learning_rate\": 0.001,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f2cd949b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 111;\n",
       "                var nbb_unformatted_code = \"import numpy as np\\n\\n\\ndef compute_r(y, y_pred):\\n    corr = np.corrcoef(y, y_pred)\\n    return corr\";\n",
       "                var nbb_formatted_code = \"import numpy as np\\n\\n\\ndef compute_r(y, y_pred):\\n    corr = np.corrcoef(y, y_pred)\\n    return corr\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_r(y, y_pred):\n",
    "    corr = np.corrcoef(y, y_pred)\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3b4788a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[2.0693]], device='cuda:1', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 112;\n",
       "                var nbb_unformatted_code = \"model(\\\"je suis japonaise c\\u2019est officiel \\ud83e\\udd74\\\")\";\n",
       "                var nbb_formatted_code = \"model(\\\"je suis japonaise c\\u2019est officiel \\ud83e\\udd74\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model(\"je suis japonaise câ€™est officiel ðŸ¥´\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "149c985c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 0: : 0batch [00:00, ?batch/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Epoch 0: : 475batch [00:54,  8.77batch/s, loss=0.802]\n",
      "Epoch 0: : 119batch [00:01, 61.48batch/s, loss=0.663]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson's r is : [[ 1.         -0.01216974]\n",
      " [-0.01216974  1.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: : 475batch [00:55,  8.60batch/s, loss=0.709]\n",
      "Epoch 1: : 119batch [00:01, 59.90batch/s, loss=0.648]\n",
      "/soe/sgarg6/conda/envs/nlp_env/lib/python3.10/site-packages/numpy/lib/function_base.py:2853: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "/soe/sgarg6/conda/envs/nlp_env/lib/python3.10/site-packages/numpy/lib/function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson's r is : [[nan nan]\n",
      " [nan  1.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: : 475batch [00:55,  8.50batch/s, loss=0.73] \n",
      "Epoch 2: : 119batch [00:01, 62.49batch/s, loss=0.644]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson's r is : [[ 1.         -0.02786552]\n",
      " [-0.02786552  1.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: : 475batch [00:55,  8.61batch/s, loss=0.69] \n",
      "Epoch 3: : 119batch [00:01, 61.32batch/s, loss=0.636]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson's r is : [[nan nan]\n",
      " [nan  1.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: : 475batch [00:54,  8.64batch/s, loss=0.72] \n",
      "Epoch 4: : 119batch [00:01, 61.15batch/s, loss=0.627]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson's r is : [[nan nan]\n",
      " [nan  1.]]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 113;\n",
       "                var nbb_unformatted_code = \"import time\\n\\nmodel = MultiLingualModel(\\\"cardiffnlp/twitter-xlm-roberta-base\\\")\\nmodel_args = get_model_args()\\n# Loss and Optimization\\nmodel_args[\\\"criterion\\\"] = nn.MSELoss()\\nmodel_args[\\\"optimizer\\\"] = torch.optim.Adam(\\n    model.model.parameters(), lr=model_args[\\\"learning_rate\\\"]\\n)\\n\\n# Log Metrics\\nepoch_train_loss = []\\nepoch_valid_loss = []\\nepoch_valid_r = []\\n# Begin Training\\nfor epoch in range(model_args[\\\"epoch\\\"]):\\n\\n    # Train the model\\n    train_loss, _, _ = train_or_valid(model_args, epoch, model)\\n    epoch_train_loss.append(train_loss)\\n\\n    # validate the model\\n    valid_loss, valid_y_pred, valid_y = train_or_valid(model_args, epoch, model, False)\\n    print(f\\\"Pearson's r is : {compute_r(valid_y_pred, valid_y)}\\\")\\n    epoch_valid_loss.append(valid_loss)\\n    epoch_valid_r.append(compute_r)\";\n",
       "                var nbb_formatted_code = \"import time\\n\\nmodel = MultiLingualModel(\\\"cardiffnlp/twitter-xlm-roberta-base\\\")\\nmodel_args = get_model_args()\\n# Loss and Optimization\\nmodel_args[\\\"criterion\\\"] = nn.MSELoss()\\nmodel_args[\\\"optimizer\\\"] = torch.optim.Adam(\\n    model.model.parameters(), lr=model_args[\\\"learning_rate\\\"]\\n)\\n\\n# Log Metrics\\nepoch_train_loss = []\\nepoch_valid_loss = []\\nepoch_valid_r = []\\n# Begin Training\\nfor epoch in range(model_args[\\\"epoch\\\"]):\\n\\n    # Train the model\\n    train_loss, _, _ = train_or_valid(model_args, epoch, model)\\n    epoch_train_loss.append(train_loss)\\n\\n    # validate the model\\n    valid_loss, valid_y_pred, valid_y = train_or_valid(model_args, epoch, model, False)\\n    print(f\\\"Pearson's r is : {compute_r(valid_y_pred, valid_y)}\\\")\\n    epoch_valid_loss.append(valid_loss)\\n    epoch_valid_r.append(compute_r)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "model = MultiLingualModel(\"cardiffnlp/twitter-xlm-roberta-base\")\n",
    "model_args = get_model_args()\n",
    "# Loss and Optimization\n",
    "model_args[\"criterion\"] = nn.MSELoss()\n",
    "model_args[\"optimizer\"] = torch.optim.Adam(\n",
    "    model.model.parameters(), lr=model_args[\"learning_rate\"]\n",
    ")\n",
    "\n",
    "# Log Metrics\n",
    "epoch_train_loss = []\n",
    "epoch_valid_loss = []\n",
    "epoch_valid_r = []\n",
    "# Begin Training\n",
    "for epoch in range(model_args[\"epoch\"]):\n",
    "\n",
    "    # Train the model\n",
    "    train_loss, _, _ = train_or_valid(model_args, epoch, model)\n",
    "    epoch_train_loss.append(train_loss)\n",
    "\n",
    "    # validate the model\n",
    "    valid_loss, valid_y_pred, valid_y = train_or_valid(model_args, epoch, model, False)\n",
    "    print(f\"Pearson's r is : {compute_r(valid_y_pred, valid_y)}\")\n",
    "    epoch_valid_loss.append(valid_loss)\n",
    "    epoch_valid_r.append(compute_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "cef9b110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[2.0599]], device='cuda:1', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 114;\n",
       "                var nbb_unformatted_code = \"model(\\\"je suis japonaise c\\u2019est officiel \\ud83e\\udd74\\\")\";\n",
       "                var nbb_formatted_code = \"model(\\\"je suis japonaise c\\u2019est officiel \\ud83e\\udd74\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model(\"je suis japonaise câ€™est officiel ðŸ¥´\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3acc719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bfea5e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 70;\n",
       "                var nbb_unformatted_code = \"import scipy.stats as stats\";\n",
       "                var nbb_formatted_code = \"import scipy.stats as stats\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5c8fe505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PearsonRResult(statistic=0.012061720920868853, pvalue=0.5993796146090237)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 109;\n",
       "                var nbb_unformatted_code = \"stats.pearsonr(valid_y, valid_y_pred)\";\n",
       "                var nbb_formatted_code = \"stats.pearsonr(valid_y, valid_y_pred)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stats.pearsonr(valid_y, valid_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "278374b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PearsonRResult(statistic=1.0, pvalue=1.0)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 73;\n",
       "                var nbb_unformatted_code = \"stats.pearsonr([1, 2], [2, 3])\";\n",
       "                var nbb_formatted_code = \"stats.pearsonr([1, 2], [2, 3])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stats.pearsonr([1, 2], [2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "120ccea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1., nan],\n",
       "       [nan, nan]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 69;\n",
       "                var nbb_unformatted_code = \"corr\";\n",
       "                var nbb_formatted_code = \"corr\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ec467301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[1.7254]], device='cuda:1', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 76;\n",
       "                var nbb_unformatted_code = \"model(\\\"Posting some VIP client tickets: http\\\")\";\n",
       "                var nbb_formatted_code = \"model(\\\"Posting some VIP client tickets: http\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model(\"Posting some VIP client tickets: http\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aff232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f8203e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import time\n",
    "\n",
    "total_loss = []\n",
    "model.model.train()\n",
    "for epoch in range(5):\n",
    "    epoch_loss = []\n",
    "    train_data = data_loader(data)\n",
    "    for X, y in train_data:\n",
    "\n",
    "        y = y.float().to(DEVICE)\n",
    "        y_pred = model(X)\n",
    "        loss = mse_loss(y_pred[\"logits\"].reshape(-1), y)\n",
    "        optimizer.zero_grad()\n",
    "        epoch_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        del X, y, y_pred, loss\n",
    "        # del y, y_pred, loss\n",
    "        import gc\n",
    "\n",
    "        gc.collect()\n",
    "        with torch.cuda.device(DEVICE):\n",
    "            torch.cuda.empty_cache()\n",
    "    total_loss.append(sum(epoch_loss) / len(epoch_loss))\n",
    "    print(total_loss)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
