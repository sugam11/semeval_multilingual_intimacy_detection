{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install demoji\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "p7e5cG4VVfJi"
      },
      "id": "p7e5cG4VVfJi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ce3233e",
      "metadata": {
        "id": "6ce3233e"
      },
      "outputs": [],
      "source": [
        "# Set seed for reproducibility\n",
        "\n",
        "import torch\n",
        "torch.manual_seed(0)\n",
        "\n",
        "import random\n",
        "random.seed(0)\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6eb0c6c6",
      "metadata": {
        "id": "6eb0c6c6"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    dev = \"cuda:0\"\n",
        "else:\n",
        "    dev = \"cpu\"\n",
        "DEVICE = torch.device(dev)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "W6pw6JDtWtWL"
      },
      "id": "W6pw6JDtWtWL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change this home path\n",
        "home_path = \"/content/drive/MyDrive/NLP 243/project\""
      ],
      "metadata": {
        "id": "AQzUNOCtZUpd"
      },
      "id": "AQzUNOCtZUpd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = {\n",
        "    # Add a linear layer before the final layer\n",
        "    \"add_linear\": False, \n",
        "    #for training on original train set: original\n",
        "    #for training on translation augmented train set: translated\n",
        "    \"train_on\": \"original\",\n",
        "    \"use_demoji\": True,\n",
        "    \"remove_mentions\": True,\n",
        "    \"remove_numbers\": True,\n",
        "    \"remove_http\": True,\n",
        "    \"stratify_split\": True,\n",
        "    \"train_zero_shot\": True\n",
        "}"
      ],
      "metadata": {
        "id": "mjK1bFB1Wh21"
      },
      "id": "mjK1bFB1Wh21",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fa29423",
      "metadata": {
        "id": "9fa29423"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, XLMRobertaModel\n",
        "\n",
        "\n",
        "class MultiLingualModel(nn.Module):\n",
        "    def __init__(self, model_name, add_linear=False):\n",
        "        super().__init__()\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = XLMRobertaModel.from_pretrained(\n",
        "            model_name, output_attentions=False, output_hidden_states=False\n",
        "        ).to(DEVICE)\n",
        "        self.add_linear = add_linear\n",
        "        if self.add_linear:\n",
        "            self.linear = nn.Sequential(nn.Dropout(0.2), nn.ReLU(), nn.Linear(768, 768)).to(\n",
        "                DEVICE\n",
        "            )\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Dropout(0.1), nn.ReLU(), nn.Linear(768, 1)\n",
        "        ).to(DEVICE)\n",
        "\n",
        "    def forward(self, sentences):\n",
        "        encoded_input = self.tokenizer(\n",
        "            sentences,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=512,\n",
        "        ).to(DEVICE)\n",
        "        out = self.model(**encoded_input)[1]\n",
        "        if self.add_linear:\n",
        "            out = self.linear(out)\n",
        "        out = self.regressor(out)\n",
        "        return out, encoded_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a8979c1",
      "metadata": {
        "id": "4a8979c1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "if training_args[\"train_on\"] == \"original\":\n",
        "    data = pd.read_csv(f\"{home_path}/data/train.csv\")\n",
        "else: \n",
        "    data = pd.read_csv(f\"{home_path}/data/full_translate_all.tsv\", sep=\"\\t\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "JzbdD3AlZ_XM"
      },
      "id": "JzbdD3AlZ_XM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a519b6b",
      "metadata": {
        "id": "1a519b6b"
      },
      "outputs": [],
      "source": [
        "# Data Cleaning Actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bd05b67",
      "metadata": {
        "id": "7bd05b67"
      },
      "outputs": [],
      "source": [
        "import demoji\n",
        "import re\n",
        "\n",
        "def handle_emoji(x):\n",
        "    x = demoji.replace_with_desc(x)\n",
        "    return re.sub(r\":\", \" \", x)\n",
        "\n",
        "# Strip leading and trailing inverted commas\n",
        "data[\"text\"] = data[\"text\"].apply(lambda x: x.strip(\"'\"))\n",
        "\n",
        "if training_args[\"use_demoji\"]:\n",
        "    # Expand emojis with description using demoji library\n",
        "    data[\"text\"] = data[\"text\"].apply(lambda x: handle_emoji(x))\n",
        "\n",
        "if training_args[\"remove_mentions\"]:\n",
        "    # get rid of mentions @user @whatever\n",
        "    data[\"text\"] = data[\"text\"].str.replace(r\"@[A-Za-z0-9_]+\", \"\", regex=True)\n",
        "\n",
        "if training_args[\"remove_numbers\"]:\n",
        "    # remove words containing numbers\n",
        "    data[\"text\"] = data[\"text\"].str.replace(r\"\\w*\\d\\w*\", \"\", regex=True)\n",
        "\n",
        "if training_args[\"remove_http\"]:\n",
        "    data[\"text\"] = data[\"text\"].str.replace(\"\\shttps?\\s\", \"\", regex=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ddb457c",
      "metadata": {
        "id": "9ddb457c"
      },
      "outputs": [],
      "source": [
        "def get_data_loader(data, batch_size=16, train=True):\n",
        "    if train:\n",
        "        shuffled_data = data.sample(frac=1, random_state=0).reset_index(drop=True)\n",
        "    else:\n",
        "        shuffled_data = data\n",
        "    start = 0\n",
        "    end = start + batch_size\n",
        "    data_len = len(shuffled_data)\n",
        "    while start < data_len:\n",
        "        sub_data = shuffled_data[start:end]\n",
        "        start += batch_size\n",
        "        end = min(start + batch_size, data_len)\n",
        "        yield sub_data[\"text\"].tolist(), torch.tensor(sub_data[\"label\"].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f05575b5",
      "metadata": {
        "id": "f05575b5"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_data, valid_data = train_test_split(\n",
        "    data,\n",
        "    test_size=0.2,\n",
        "    shuffle=True,\n",
        "    random_state=0,\n",
        "    stratify=data[\"language\"] if training_args[\"stratify_split\"] else None,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cfebdc5",
      "metadata": {
        "id": "5cfebdc5"
      },
      "outputs": [],
      "source": [
        "if training_args[\"train_zero_shot\"]:\n",
        "    train_data = train_data[\n",
        "        ~train_data[\"language\"].isin([\"Korean\", \"Dutch\", \"Arabic\", \"Hindi\"])\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32d2fca4",
      "metadata": {
        "id": "32d2fca4"
      },
      "outputs": [],
      "source": [
        "train_data.language.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01aed6fa",
      "metadata": {
        "id": "01aed6fa"
      },
      "outputs": [],
      "source": [
        "valid_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd603dcc",
      "metadata": {
        "id": "dd603dcc"
      },
      "outputs": [],
      "source": [
        "train_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe00ef97",
      "metadata": {
        "id": "fe00ef97"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def train_or_valid(model_args, curr_epoch, model, is_train=True):\n",
        "    \"\"\"\n",
        "    This fn. is used to train or validate the model\n",
        "    params:\n",
        "        model_args: a dict of model parameters\n",
        "        curr_epoch: Current value of the epoch\n",
        "        model: model to be trained\n",
        "        is_train: can be True or False depending on whether to train or validate\n",
        "\n",
        "    returns:\n",
        "        loss: sum of the loss across all tokens\n",
        "\n",
        "    \"\"\"\n",
        "    loss_list = []\n",
        "    y_pred_list = []\n",
        "    y_list = []\n",
        "    model_args[\"optimizer\"].zero_grad()\n",
        "    train_type = None\n",
        "    if is_train:\n",
        "        data_loader = get_data_loader(train_data, batch_size=model_args[\"batch_size\"])\n",
        "        model.train()\n",
        "        train_type = \"train\"\n",
        "    else:\n",
        "        data_loader = get_data_loader(\n",
        "            valid_data, batch_size=model_args[\"batch_size\"], train=False\n",
        "        )\n",
        "        model.eval()\n",
        "        train_type = \"valid\"\n",
        "\n",
        "    with tqdm(data_loader, unit=\"batch\") as tepoch:\n",
        "        tepoch.set_description(f\"Epoch {curr_epoch} - {train_type}\")\n",
        "        for step, batch in enumerate(tepoch):\n",
        "            model_args[\"optimizer\"].zero_grad()\n",
        "            X = batch[0]\n",
        "            y = batch[1].float().to(DEVICE)\n",
        "            y_pred, _ = model(X)\n",
        "            y_pred_list.extend(y_pred.reshape(-1).tolist())\n",
        "            y_list.extend(y.tolist())\n",
        "            loss = model_args[\"criterion\"](y_pred.reshape(-1), y)\n",
        "            loss_list.append(loss.item())\n",
        "            if is_train:\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_norm_(model.parameters(), 2)\n",
        "                model_args[\"optimizer\"].step()\n",
        "                model_args[\"scheduler\"].step()\n",
        "                with torch.cuda.device(DEVICE):\n",
        "                    torch.cuda.empty_cache()\n",
        "            tepoch.set_postfix(loss=sum(loss_list) / len(loss_list))\n",
        "    if is_train is False:\n",
        "        valid_data[f\"y_pred_{curr_epoch}\"] = y_pred_list\n",
        "    else:\n",
        "        train_data[f\"y_pred_{curr_epoch}\"] = y_pred_list\n",
        "    return sum(loss_list) / len(loss_list), y_pred_list, y_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd559183",
      "metadata": {
        "id": "fd559183"
      },
      "outputs": [],
      "source": [
        "# Defining parameters for training the model\n",
        "def get_model_args():\n",
        "    # returns a dict - {param: value}\n",
        "    return {\n",
        "        \"batch_size\": 64,\n",
        "        \"epoch\": 15,\n",
        "        \"learning_rate\": 0.0001,\n",
        "        \"model_name\": \"cardiffnlp/twitter-xlm-roberta-base\",\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4713c6d",
      "metadata": {
        "id": "d4713c6d"
      },
      "outputs": [],
      "source": [
        "import scipy\n",
        "\n",
        "def compute_r(y, y_pred):\n",
        "    corr = scipy.stats.pearsonr(y, y_pred)\n",
        "    return corr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e66d93e6",
      "metadata": {
        "id": "e66d93e6"
      },
      "outputs": [],
      "source": [
        "# Compute and store pearsons score every epoch for each language\n",
        "\n",
        "def reset_language_score():\n",
        "    language_score = {}\n",
        "    for language in valid_data[\"language\"].unique():\n",
        "        language_score[language] = []\n",
        "    return language_score\n",
        "\n",
        "\n",
        "def compute_language_correlation(valid_data, epoch, language_score):\n",
        "    for language in valid_data[\"language\"].unique():\n",
        "        r = compute_r(\n",
        "            valid_data[valid_data[\"language\"] == language][f\"y_pred_{epoch}\"],\n",
        "            valid_data[valid_data[\"language\"] == language][\"label\"],\n",
        "        )\n",
        "        print(f\"correlation for {language} is : {r}\")\n",
        "        language_score[language].append(r[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4a0b2e7",
      "metadata": {
        "scrolled": false,
        "id": "c4a0b2e7"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "model_args = get_model_args()\n",
        "model = MultiLingualModel(model_args[\"model_name\"])\n",
        "\n",
        "# Loss and Optimization\n",
        "total_steps = (len(train_data) // (model_args[\"batch_size\"]) + 1) * model_args[\"epoch\"]\n",
        "model_args[\"criterion\"] = nn.MSELoss()\n",
        "model_args[\"optimizer\"] = AdamW(\n",
        "    model.parameters(), lr=model_args[\"learning_rate\"], eps=1e-8\n",
        ")\n",
        "model_args[\"scheduler\"] = get_linear_schedule_with_warmup(\n",
        "    model_args[\"optimizer\"], num_warmup_steps=0, num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "language_score = reset_language_score()\n",
        "# Log Metrics\n",
        "epoch_train_loss = []\n",
        "epoch_valid_loss = []\n",
        "epoch_valid_r = []\n",
        "\n",
        "# validate the model\n",
        "valid_loss, valid_y_pred, valid_y = train_or_valid(model_args, 0, model, False)\n",
        "compute_language_correlation(valid_data, 0, language_score)\n",
        "\n",
        "# Begin Training\n",
        "for epoch in range(model_args[\"epoch\"]):\n",
        "\n",
        "    # Train the model\n",
        "    train_loss, _, _ = train_or_valid(model_args, epoch, model)\n",
        "    epoch_train_loss.append(train_loss)\n",
        "    with torch.cuda.device(DEVICE):\n",
        "        torch.cuda.empty_cache()\n",
        "    # validate the model\n",
        "    valid_loss, valid_y_pred, valid_y = train_or_valid(model_args, epoch, model, False)\n",
        "    compute_language_correlation(valid_data, epoch, language_score)\n",
        "    epoch_valid_loss.append(valid_loss)\n",
        "    epoch_valid_r.append(compute_r)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5d47f8b",
      "metadata": {
        "id": "f5d47f8b"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "xi = list(range(model_args[\"epoch\"]))\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 5)\n",
        "for lang, score in language_score.items():\n",
        "    plt.plot(score, label=f\"pearson_{lang}\")\n",
        "\n",
        "plt.xticks(xi, range(model_args[\"epoch\"]))\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Pearson's Score\")\n",
        "plt.title(\"Epoch vs Pearson's Score\")\n",
        "plt.legend(fancybox=True, shadow=True)\n",
        "plt.savefig(f\"images/pearson_score-xlmt_base-6lang.png\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51cb766f",
      "metadata": {
        "id": "51cb766f"
      },
      "outputs": [],
      "source": [
        "valid_data[\"diff\"] = valid_data.apply(\n",
        "    lambda x: abs(x[\"y_pred_14\"] - x[\"label\"]), axis=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4be2da66",
      "metadata": {
        "id": "4be2da66"
      },
      "outputs": [],
      "source": [
        "valid_data.sort_values(\"diff\", ascending=False).to_csv(\"xlm_valid.csv\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}